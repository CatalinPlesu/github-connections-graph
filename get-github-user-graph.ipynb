{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12e90e87-12e2-4870-9154-6db5b08b3277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 10:08:31,524 - INFO - Starting crawl from CatalinPlesu\n",
      "2025-05-28 10:08:31,527 - INFO - Max depth: 5, Max pages: 1000\n",
      "2025-05-28 10:08:31,529 - INFO - Timeouts - Page load: 30s, Element wait: 15s\n",
      "2025-05-28 10:08:31,531 - INFO - Crawling CatalinPlesu at depth 1\n",
      "2025-05-28 10:08:31,532 - INFO - Loading https://github.com/CatalinPlesu (attempt 1/3)\n",
      "2025-05-28 10:09:01,542 - WARNING - Timeout loading https://github.com/CatalinPlesu on attempt 1\n",
      "2025-05-28 10:09:04,658 - INFO - Loading https://github.com/CatalinPlesu (attempt 2/3)\n",
      "2025-05-28 10:09:23,791 - INFO - Crawling interrupted by user\n",
      "2025-05-28 10:09:23,795 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x751f9a748bb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8f48a4e3-f9cd-4aea-b02b-9d67ae697377\n",
      "2025-05-28 10:09:23,798 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x751f998f65e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8f48a4e3-f9cd-4aea-b02b-9d67ae697377\n",
      "2025-05-28 10:09:23,802 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x751f9950c7c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8f48a4e3-f9cd-4aea-b02b-9d67ae697377\n",
      "2025-05-28 10:09:23,805 - INFO - Browser closed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "START_USER = \"CatalinPlesu\"\n",
    "MAX_DEPTH = 5\n",
    "MAX_PAGES = 1000\n",
    "MAX_FOLLOWERS_FOLLOWING = 25\n",
    "WHITELIST = [START_USER, \"Ernest96\"]\n",
    "FAIR_LIMIT = 10\n",
    "DATA_FILE = \"github_users.json\"\n",
    "IMG_FOLDER = \"img\"\n",
    "\n",
    "# Timeout settings\n",
    "PAGE_LOAD_TIMEOUT = 30  # seconds\n",
    "ELEMENT_WAIT_TIMEOUT = 15  # seconds\n",
    "REQUEST_TIMEOUT = 20  # seconds for image downloads\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Setup Firefox driver with timeout configurations\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Create driver\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    \n",
    "    # Set timeouts\n",
    "    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "    driver.implicitly_wait(ELEMENT_WAIT_TIMEOUT)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def safe_get_page(driver, url, max_retries=3):\n",
    "    \"\"\"Safely load a page with retries and timeout handling\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logger.info(f\"Loading {url} (attempt {attempt + 1}/{max_retries})\")\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for page to be ready\n",
    "            WebDriverWait(driver, ELEMENT_WAIT_TIMEOUT).until(\n",
    "                lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "            )\n",
    "            \n",
    "            # Add random delay to avoid being too aggressive\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            return True\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.warning(f\"Timeout loading {url} on attempt {attempt + 1}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(random.uniform(3, 8))  # Wait before retry\n",
    "                continue\n",
    "                \n",
    "        except WebDriverException as e:\n",
    "            logger.error(f\"WebDriver error loading {url}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(random.uniform(3, 8))\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error loading {url}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(random.uniform(3, 8))\n",
    "                continue\n",
    "    \n",
    "    logger.error(f\"Failed to load {url} after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "def save_data(data):\n",
    "    \"\"\"Save data with error handling\"\"\"\n",
    "    try:\n",
    "        with open(DATA_FILE, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        logger.info(f\"Data saved. Total users: {len(data['users'])}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save data: {e}\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load existing data\"\"\"\n",
    "    try:\n",
    "        with open(DATA_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {\"users\": {}}\n",
    "\n",
    "def download_image(url, username):\n",
    "    \"\"\"Download image with timeout handling\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "        \n",
    "    image_path = os.path.join(IMG_FOLDER, f\"{username}.jpg\")\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, timeout=REQUEST_TIMEOUT, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return f\"./{image_path}\"\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"Timeout downloading image for {username}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to download image for {username}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_user_profile(driver, username):\n",
    "    \"\"\"Get user profile with timeout handling\"\"\"\n",
    "    url = f\"https://github.com/{username}\"\n",
    "    \n",
    "    if not safe_get_page(driver, url):\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Check if user exists\n",
    "        if \"Page not found\" in driver.page_source or \"404\" in driver.title:\n",
    "            logger.warning(f\"User {username} not found\")\n",
    "            return None, None\n",
    "        \n",
    "        # Get display name with timeout\n",
    "        display_name = None\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, ELEMENT_WAIT_TIMEOUT)\n",
    "            display_name_elem = wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".vcard-fullname\"))\n",
    "            )\n",
    "            display_name = display_name_elem.text.strip()\n",
    "        except TimeoutException:\n",
    "            logger.debug(f\"No display name found for {username}\")\n",
    "        \n",
    "        # Get profile image\n",
    "        image_url = None\n",
    "        try:\n",
    "            img_elem = driver.find_element(By.CSS_SELECTOR, \"img.avatar-user\")\n",
    "            image_url = img_elem.get_attribute(\"src\")\n",
    "        except Exception:\n",
    "            logger.debug(f\"No profile image found for {username}\")\n",
    "\n",
    "        local_image_path = download_image(image_url, username)\n",
    "        return display_name, local_image_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting profile for {username}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def get_usernames_from_list_page(driver):\n",
    "    \"\"\"Extract usernames from current page\"\"\"\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, ELEMENT_WAIT_TIMEOUT)\n",
    "        elements = wait.until(\n",
    "            EC.presence_of_all_elements_located(\n",
    "                (By.CSS_SELECTOR, \"div.d-table div:nth-child(2) > a:nth-child(1) > span:nth-child(2)\")\n",
    "            )\n",
    "        )\n",
    "        return [el.text.strip() for el in elements if el.text.strip()]\n",
    "    except TimeoutException:\n",
    "        logger.debug(\"No usernames found on current page\")\n",
    "        return []\n",
    "\n",
    "def crawl_follow_list(driver, username, tab, depth):\n",
    "    \"\"\"Crawl followers or following list with better error handling\"\"\"\n",
    "    users = []\n",
    "    page = 1\n",
    "    max_pages_per_user = 10\n",
    "    consecutive_failures = 0\n",
    "    max_consecutive_failures = 3\n",
    "    \n",
    "    while page <= max_pages_per_user and consecutive_failures < max_consecutive_failures:\n",
    "        url = f\"https://github.com/{username}?tab={tab}&page={page}\"\n",
    "        \n",
    "        if not safe_get_page(driver, url):\n",
    "            consecutive_failures += 1\n",
    "            logger.warning(f\"Failed to load page {page} for {username}'s {tab}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            page_users = get_usernames_from_list_page(driver)\n",
    "            if not page_users:\n",
    "                logger.debug(f\"No users found on page {page} for {username}'s {tab}\")\n",
    "                break\n",
    "\n",
    "            users.extend(page_users)\n",
    "            consecutive_failures = 0  # Reset failure counter\n",
    "            logger.debug(f\"Found {len(page_users)} users on page {page} of {username}'s {tab}\")\n",
    "\n",
    "            # Check for next page\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.CSS_SELECTOR, \"a.next_page\")\n",
    "                if 'disabled' in next_btn.get_attribute(\"class\"):\n",
    "                    break\n",
    "            except Exception:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            consecutive_failures += 1\n",
    "            logger.warning(f\"Error processing page {page} for {username}'s {tab}: {e}\")\n",
    "    \n",
    "    # Apply fair limit\n",
    "    if username not in WHITELIST and len(users) > MAX_FOLLOWERS_FOLLOWING:\n",
    "        logger.info(f\"Limiting {tab} for {username}: {len(users)} -> {FAIR_LIMIT} (not whitelisted)\")\n",
    "        return users[:FAIR_LIMIT]\n",
    "    \n",
    "    return users\n",
    "\n",
    "def crawl_user(driver, username, data, depth):\n",
    "    \"\"\"Crawl a single user with comprehensive error handling\"\"\"\n",
    "    if depth > MAX_DEPTH:\n",
    "        logger.info(f\"Max depth {MAX_DEPTH} reached, skipping user {username}\")\n",
    "        return False\n",
    "        \n",
    "    if username in data[\"users\"]:\n",
    "        logger.debug(f\"User {username} already crawled\")\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Crawling {username} at depth {depth}\")\n",
    "        \n",
    "        display_name, image = get_user_profile(driver, username)\n",
    "        if display_name is None and image is None:\n",
    "            logger.warning(f\"Could not get profile for {username}\")\n",
    "            return False\n",
    "            \n",
    "        followers = crawl_follow_list(driver, username, \"followers\", depth)\n",
    "        following = crawl_follow_list(driver, username, \"following\", depth)\n",
    "\n",
    "        data[\"users\"][username] = {\n",
    "            \"username\": username,\n",
    "            \"displayName\": display_name,\n",
    "            \"image\": image,\n",
    "            \"followers\": followers,\n",
    "            \"following\": following,\n",
    "            \"depth\": depth,\n",
    "            \"crawled_at\": time.time()\n",
    "        }\n",
    "        \n",
    "        whitelist_status = \" (whitelisted)\" if username in WHITELIST else \"\"\n",
    "        logger.info(f\"Successfully crawled {username} at depth {depth}{whitelist_status}: \"\n",
    "                   f\"followers={len(followers)}, following={len(following)}\")\n",
    "        \n",
    "        save_data(data)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to crawl {username}: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with better resource management\"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        # Setup\n",
    "        driver = setup_driver()\n",
    "        os.makedirs(IMG_FOLDER, exist_ok=True)\n",
    "        \n",
    "        data = load_data()\n",
    "        pages_visited = 0\n",
    "        failed_users = set()\n",
    "\n",
    "        current_level = [START_USER]\n",
    "        next_level = []\n",
    "        depth = 1\n",
    "        crawled = set(data[\"users\"].keys())\n",
    "\n",
    "        logger.info(f\"Starting crawl from {START_USER}\")\n",
    "        logger.info(f\"Max depth: {MAX_DEPTH}, Max pages: {MAX_PAGES}\")\n",
    "        logger.info(f\"Timeouts - Page load: {PAGE_LOAD_TIMEOUT}s, Element wait: {ELEMENT_WAIT_TIMEOUT}s\")\n",
    "\n",
    "        while current_level and pages_visited < MAX_PAGES and depth <= MAX_DEPTH:\n",
    "            user = current_level.pop(0)\n",
    "            \n",
    "            if user in crawled or user in failed_users:\n",
    "                continue\n",
    "\n",
    "            success = crawl_user(driver, user, data, depth)\n",
    "            \n",
    "            if success:\n",
    "                crawled.add(user)\n",
    "                pages_visited += 1\n",
    "\n",
    "                # Add neighbors to next level\n",
    "                if depth < MAX_DEPTH:\n",
    "                    user_data = data[\"users\"].get(user, {})\n",
    "                    neighbors = user_data.get(\"followers\", []) + user_data.get(\"following\", [])\n",
    "                    for neighbor in neighbors:\n",
    "                        if (neighbor not in crawled and \n",
    "                            neighbor not in failed_users and\n",
    "                            neighbor not in current_level and \n",
    "                            neighbor not in next_level):\n",
    "                            next_level.append(neighbor)\n",
    "            else:\n",
    "                failed_users.add(user)\n",
    "                logger.warning(f\"Added {user} to failed users list\")\n",
    "\n",
    "            # Move to next depth level\n",
    "            if not current_level and next_level:\n",
    "                current_level, next_level = next_level, []\n",
    "                depth += 1\n",
    "                logger.info(f\"Moving to depth {depth}, {len(current_level)} users queued\")\n",
    "\n",
    "            # Progress reporting\n",
    "            if pages_visited % 5 == 0:  # More frequent reporting\n",
    "                logger.info(f\"Progress - Pages: {pages_visited}, Depth: {depth}, \"\n",
    "                          f\"Current queue: {len(current_level)}, Next queue: {len(next_level)}, \"\n",
    "                          f\"Failed: {len(failed_users)}\")\n",
    "\n",
    "        logger.info(f\"Crawling complete! Pages visited: {pages_visited}, \"\n",
    "                   f\"Final depth: {depth-1}, Failed users: {len(failed_users)}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Crawling interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "                logger.info(\"Browser closed\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error closing browser: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db509a3f-1420-4337-b7cb-72b5d48fbc8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'github_users.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m DATA_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub_users.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m OUTPUT_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph-data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDATA_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     raw_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m users \u001b[38;5;241m=\u001b[39m raw_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Workspace/Jupiter/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'github_users.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "DATA_FILE = \"github_users.json\"\n",
    "OUTPUT_FILE = \"graph-data.json\"\n",
    "\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "users = raw_data[\"users\"]\n",
    "\n",
    "nodes = []\n",
    "usernames_seen = set()\n",
    "username_to_depth = {}\n",
    "\n",
    "# Add main users with depth\n",
    "for username, info in users.items():\n",
    "    depth = info.get(\"depth\", 3)\n",
    "    nodes.append({\n",
    "        \"id\": username,\n",
    "        \"name\": info.get(\"displayName\", username),\n",
    "        \"img\": info.get(\"image\", \"\"),\n",
    "        \"url\": f\"https://github.com/{username}\",\n",
    "        \"depth\": depth\n",
    "    })\n",
    "    usernames_seen.add(username)\n",
    "    username_to_depth[username] = depth\n",
    "\n",
    "# Determine current max depth\n",
    "max_depth = max(username_to_depth.values(), default=3)\n",
    "\n",
    "# Add followers if not already included\n",
    "for info in users.values():\n",
    "    for follower in info.get(\"followers\", []):\n",
    "        if follower not in usernames_seen:\n",
    "            follower_depth = max_depth + 1\n",
    "            nodes.append({\n",
    "                \"id\": follower,\n",
    "                \"name\": follower,\n",
    "                \"img\": f\"./src/{follower}.jpg\",\n",
    "                \"url\": f\"https://github.com/{follower}\",\n",
    "                \"depth\": follower_depth\n",
    "            })\n",
    "            usernames_seen.add(follower)\n",
    "            username_to_depth[follower] = follower_depth\n",
    "\n",
    "# Create edges\n",
    "links = []\n",
    "for username, info in users.items():\n",
    "    for follower in info.get(\"followers\", []):\n",
    "        links.append({\n",
    "            \"source\": follower,\n",
    "            \"target\": username\n",
    "        })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"nodes\": nodes, \"links\": links}, f, indent=2)\n",
    "\n",
    "print(f\"Graph data written to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd0c6f9e-2e44-41fb-81ec-0707d7db68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve the file for the html to be able to load the json\n",
    "# For Python 3.x\n",
    "#python -m http.server 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98da28e-d0fa-45e7-bb0e-a0fd209c36a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
