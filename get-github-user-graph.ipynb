{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12e90e87-12e2-4870-9154-6db5b08b3277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawled CatalinPlesu at depth 1 (whitelisted): followers=28, following=50\n",
      "Limiting followers for mi6paulino: 50 -> 10 (not in whitelist)\n",
      "Limiting following for mi6paulino: 50 -> 10 (not in whitelist)\n",
      "Crawled mi6paulino at depth 2: followers=10, following=10\n",
      "Crawled DenisMunjiu at depth 2: followers=1, following=1\n",
      "Crawled BeginnerDuelist at depth 2: followers=6, following=11\n",
      "Crawled c-harea at depth 2: followers=4, following=4\n",
      "Crawled UzunPaula at depth 2: followers=6, following=21\n",
      "Limiting following for sergiuprt: 50 -> 10 (not in whitelist)\n",
      "Crawled sergiuprt at depth 2: followers=18, following=10\n",
      "Limiting followers for Raduc4: 39 -> 10 (not in whitelist)\n",
      "Limiting following for Raduc4: 50 -> 10 (not in whitelist)\n",
      "Crawled Raduc4 at depth 2: followers=10, following=10\n",
      "Limiting following for AnnHR: 50 -> 10 (not in whitelist)\n",
      "Crawled AnnHR at depth 2: followers=5, following=10\n",
      "Crawled Diana1407c at depth 2: followers=1, following=5\n",
      "############################################################################\n",
      "Pages visited: 10\n",
      "Current depth: 2\n",
      "Avreage duration: 7.023038339614868s/user\n",
      "Users remaining in current_level queue: 43 (~302.0s)\n",
      "Users remaining in next_level queue: 109 (~765.5s)\n",
      "Manual estimate: ~912.0s\n",
      "############################################################################\n",
      "Crawled Vlardian at depth 2: followers=1, following=1\n",
      "Crawled xRuBYN at depth 2: followers=5, following=6\n",
      "Crawled BarganConstantin at depth 2: followers=5, following=9\n",
      "Crawled ecaterinast at depth 2: followers=5, following=9\n",
      "Crawled Neo7Noir at depth 2: followers=4, following=5\n",
      "Crawled ApaAura at depth 2: followers=11, following=10\n",
      "Crawled DoruApareci at depth 2: followers=14, following=11\n",
      "Crawled xccelerator at depth 2: followers=11, following=11\n",
      "Crawled katolandau at depth 2: followers=2, following=9\n",
      "Crawled MardariSandu at depth 2: followers=1, following=1\n",
      "############################################################################\n",
      "Pages visited: 20\n",
      "Current depth: 2\n",
      "Avreage duration: 6.806888246536255s/user\n",
      "Users remaining in current_level queue: 33 (~224.6s)\n",
      "Users remaining in next_level queue: 156 (~1061.9s)\n",
      "Manual estimate: ~1134.0s\n",
      "############################################################################\n",
      "Crawled StefanVlasitchi at depth 2: followers=1, following=3\n",
      "Limiting followers for BabaDorin: 32 -> 10 (not in whitelist)\n",
      "Limiting following for BabaDorin: 28 -> 10 (not in whitelist)\n",
      "Crawled BabaDorin at depth 2: followers=10, following=10\n",
      "Crawled 1Leomas at depth 2: followers=5, following=6\n",
      "Crawled paradise2003 at depth 2: followers=1, following=5\n",
      "Crawled Nicucenicu at depth 2: followers=2, following=2\n",
      "Crawled ChloePrice4Ever at depth 2: followers=2, following=3\n",
      "Crawled Midn18 at depth 2: followers=10, following=10\n",
      "Crawled MelissaD998 at depth 2: followers=5, following=6\n",
      "Crawled nicoleta7688 at depth 2: followers=10, following=6\n",
      "Crawled GheorgheMorari at depth 2: followers=16, following=25\n",
      "############################################################################\n",
      "Pages visited: 30\n",
      "Current depth: 2\n",
      "Avreage duration: 6.700879947344462s/user\n",
      "Users remaining in current_level queue: 23 (~154.1s)\n",
      "Users remaining in next_level queue: 212 (~1420.6s)\n",
      "Manual estimate: ~1410.0s\n",
      "############################################################################\n",
      "Limiting followers for Xe: 50 -> 10 (not in whitelist)\n",
      "Limiting following for Xe: 50 -> 10 (not in whitelist)\n",
      "Crawled Xe at depth 2: followers=10, following=10\n",
      "Limiting followers for cloudofoz: 50 -> 10 (not in whitelist)\n",
      "Crawled cloudofoz at depth 2: followers=10, following=12\n",
      "Limiting followers for 2Retr0: 50 -> 10 (not in whitelist)\n",
      "Crawled 2Retr0 at depth 2: followers=10, following=3\n",
      "Crawled saneag at depth 2: followers=8, following=3\n",
      "Crawled ioneltuc at depth 2: followers=5, following=4\n",
      "Crawled DimaMihaes at depth 2: followers=3, following=3\n",
      "Crawled vcecan at depth 2: followers=4, following=10\n",
      "Crawled BunescuGabriel at depth 2: followers=4, following=2\n",
      "Crawled rusucatalin at depth 2: followers=9, following=11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 215\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawling complete. Total pages visited: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpages_visited\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Final depth reached: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 167\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    166\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 167\u001b[0m \u001b[43mcrawl_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m durations\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m    170\u001b[0m crawled\u001b[38;5;241m.\u001b[39madd(user)\n",
      "Cell \u001b[0;32mIn[29], line 127\u001b[0m, in \u001b[0;36mcrawl_user\u001b[0;34m(username, data, depth)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already crawled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m display_name, image \u001b[38;5;241m=\u001b[39m \u001b[43mget_user_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m followers \u001b[38;5;241m=\u001b[39m crawl_follow_list(username, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowers\u001b[39m\u001b[38;5;124m\"\u001b[39m, depth)\n\u001b[1;32m    129\u001b[0m following \u001b[38;5;241m=\u001b[39m crawl_follow_list(username, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowing\u001b[39m\u001b[38;5;124m\"\u001b[39m, depth)\n",
      "Cell \u001b[0;32mIn[29], line 89\u001b[0m, in \u001b[0;36mget_user_profile\u001b[0;34m(username)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     image_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m local_image_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m display_name, local_image_path\n",
      "Cell \u001b[0;32mIn[29], line 70\u001b[0m, in \u001b[0;36mdownload_image\u001b[0;34m(url, username)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(image_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     69\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCRAWL_DELAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from statistics import mean\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Constants\n",
    "START_USER = \"CatalinPlesu\"\n",
    "MAX_DEPTH = 5\n",
    "MAX_PAGES = 1000\n",
    "MAX_FOLLOWERS_FOLLOWING = 25\n",
    "FAIR_LIMIT = 10\n",
    "WHITELIST = [START_USER, \"Ernest96\"]\n",
    "CRAWL_DELAY = 1\n",
    "ESTIMATE_PAGES_PER_USER = 3\n",
    "ESTIMATE_DELAY_PER_PAGE = 2\n",
    "\n",
    "DATA_FILE = \"github_users.json\"\n",
    "STATE_FILE = \"crawler_state.json\"\n",
    "IMG_FOLDER = \"img\"\n",
    "\n",
    "# Setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=options)\n",
    "os.makedirs(IMG_FOLDER, exist_ok=True)\n",
    "\n",
    "# Persistence\n",
    "def save_data(data):\n",
    "    with open(DATA_FILE, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        with open(DATA_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {\"users\": {}}\n",
    "\n",
    "def save_state(state):\n",
    "    with open(STATE_FILE, \"w\") as f:\n",
    "        json.dump(state, f, indent=2)\n",
    "\n",
    "def load_state():\n",
    "    try:\n",
    "        with open(STATE_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {\n",
    "            \"depth\": 1,\n",
    "            \"pages_visited\": 0,\n",
    "            \"current_level\": [START_USER],\n",
    "            \"next_level\": [],\n",
    "            \"durations\": []\n",
    "        }\n",
    "\n",
    "# Image Download\n",
    "def download_image(url, username):\n",
    "    if not url:\n",
    "        return None\n",
    "    image_path = os.path.join(IMG_FOLDER, f\"{username}.jpg\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        time.sleep(CRAWL_DELAY)\n",
    "        return f\"./{image_path}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download image for {username}: {e}\")\n",
    "        return None\n",
    "\n",
    "# GitHub Crawling Logic\n",
    "def get_user_profile(username):\n",
    "    url = f\"https://github.com/{username}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(CRAWL_DELAY)\n",
    "    try:\n",
    "        display_name = driver.find_element(By.CSS_SELECTOR, \".vcard-fullname\").text.strip()\n",
    "    except:\n",
    "        display_name = None\n",
    "    try:\n",
    "        image_url = driver.find_element(By.CSS_SELECTOR, \"img.avatar-user\").get_attribute(\"src\")\n",
    "    except:\n",
    "        image_url = None\n",
    "    local_image_path = download_image(image_url, username)\n",
    "    return display_name, local_image_path\n",
    "\n",
    "def get_usernames_from_list_page():\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, \"div.d-table div:nth-child(2) > a:nth-child(1) > span:nth-child(2)\")\n",
    "    return [el.text.strip() for el in elements]\n",
    "\n",
    "def crawl_follow_list(username, tab, depth):\n",
    "    users = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"https://github.com/{username}?tab={tab}&page={page}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(CRAWL_DELAY)\n",
    "        page_users = get_usernames_from_list_page()\n",
    "        if not page_users:\n",
    "            break\n",
    "        users.extend(page_users)\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.CSS_SELECTOR, \"a.next_page\")\n",
    "            if 'disabled' in next_btn.get_attribute(\"class\"):\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "        page += 1\n",
    "    if username not in WHITELIST and len(users) > MAX_FOLLOWERS_FOLLOWING:\n",
    "        print(f\"Limiting {tab} for {username}: {len(users)} -> {FAIR_LIMIT} (not in whitelist)\")\n",
    "        return users[:FAIR_LIMIT]\n",
    "    return users\n",
    "\n",
    "def crawl_user(username, data, depth):\n",
    "    if depth > MAX_DEPTH:\n",
    "        print(f\"Max depth {MAX_DEPTH} reached, skipping user {username}\")\n",
    "        return\n",
    "    if username in data[\"users\"]:\n",
    "        print(f\"User {username} already crawled.\")\n",
    "        return\n",
    "\n",
    "    display_name, image = get_user_profile(username)\n",
    "    followers = crawl_follow_list(username, \"followers\", depth)\n",
    "    following = crawl_follow_list(username, \"following\", depth)\n",
    "\n",
    "    data[\"users\"][username] = {\n",
    "        \"username\": username,\n",
    "        \"displayName\": display_name,\n",
    "        \"image\": image,\n",
    "        \"followers\": followers,\n",
    "        \"following\": following,\n",
    "        \"depth\": depth\n",
    "    }\n",
    "\n",
    "    whitelist_status = \" (whitelisted)\" if username in WHITELIST else \"\"\n",
    "    print(f\"Crawled {username} at depth {depth}{whitelist_status}: followers={len(followers)}, following={len(following)}\")\n",
    "    save_data(data)\n",
    "\n",
    "# Estimation Helper\n",
    "def estimate_remaining_time(users_remaining, per_user_pages=ESTIMATE_PAGES_PER_USER, delay_per_page=ESTIMATE_DELAY_PER_PAGE):\n",
    "    return users_remaining * per_user_pages * delay_per_page\n",
    "\n",
    "# Main Crawl Loop\n",
    "def main():\n",
    "    data = load_data()\n",
    "    state = load_state()\n",
    "\n",
    "    depth = state[\"depth\"]\n",
    "    pages_visited = state[\"pages_visited\"]\n",
    "    current_level = state[\"current_level\"]\n",
    "    next_level = state[\"next_level\"]\n",
    "    durations = state[\"durations\"]\n",
    "\n",
    "    crawled = set(data[\"users\"].keys())\n",
    "\n",
    "    while current_level and pages_visited < MAX_PAGES and depth <= MAX_DEPTH:\n",
    "        user = current_level.pop(0)\n",
    "        if user in crawled:\n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "        crawl_user(user, data, depth)\n",
    "        durations.append(time.time() - start_time)\n",
    "\n",
    "        crawled.add(user)\n",
    "        pages_visited += 1\n",
    "\n",
    "        if depth <= MAX_DEPTH:\n",
    "            user_data = data[\"users\"].get(user, {})\n",
    "            if not isinstance(user_data, dict):\n",
    "                print(f\"Invalid user data format for {user}, skipping.\")\n",
    "                continue\n",
    "            neighbors = user_data.get(\"followers\", []) + user_data.get(\"following\", [])\n",
    "            for u in neighbors:\n",
    "                if u not in crawled and u not in current_level and u not in next_level:\n",
    "                    next_level.append(u)\n",
    "\n",
    "        if not current_level:\n",
    "            current_level, next_level = next_level, []\n",
    "            depth += 1\n",
    "\n",
    "        # Save crawler state\n",
    "        save_state({\n",
    "            \"depth\": depth,\n",
    "            \"pages_visited\": pages_visited,\n",
    "            \"current_level\": current_level,\n",
    "            \"next_level\": next_level,\n",
    "            \"durations\": durations\n",
    "        })\n",
    "\n",
    "        # Progress report\n",
    "        if pages_visited % 10 == 0 and durations:\n",
    "            avg_time = mean(durations)\n",
    "            est_current = len(current_level) * avg_time\n",
    "            est_next = len(next_level) * avg_time\n",
    "            est_manual = estimate_remaining_time(len(current_level) + len(next_level))\n",
    "            print(\"############################################################################\")\n",
    "            print(f\"Pages visited: {pages_visited}\")\n",
    "            print(f\"Current depth: {depth}\")\n",
    "            print(f\"Avreage duration: {avg_time}s/user\")\n",
    "            print(f\"Users remaining in current_level queue: {len(current_level)} (~{est_current:.1f}s)\")\n",
    "            print(f\"Users remaining in next_level queue: {len(next_level)} (~{est_next:.1f}s)\")\n",
    "            print(f\"Manual estimate: ~{est_manual:.1f}s\")\n",
    "            print(\"############################################################################\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Crawling complete. Total pages visited: {pages_visited}, Final depth reached: {depth-1}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db509a3f-1420-4337-b7cb-72b5d48fbc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_FILE = \"github_users.json\"\n",
    "OUTPUT_FILE = \"graph-data.json\"\n",
    "\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "users = raw_data[\"users\"]\n",
    "\n",
    "nodes = []\n",
    "usernames_seen = set()\n",
    "username_to_depth = {}\n",
    "\n",
    "# Add main users with depth\n",
    "for username, info in users.items():\n",
    "    depth = info.get(\"depth\", 3)\n",
    "    nodes.append({\n",
    "        \"id\": username,\n",
    "        \"name\": info.get(\"displayName\", username),\n",
    "        \"img\": info.get(\"image\", \"\"),\n",
    "        \"url\": f\"https://github.com/{username}\",\n",
    "        \"depth\": depth\n",
    "    })\n",
    "    usernames_seen.add(username)\n",
    "    username_to_depth[username] = depth\n",
    "\n",
    "# Determine current max depth\n",
    "max_depth = max(username_to_depth.values(), default=3)\n",
    "\n",
    "# Add followers if not already included\n",
    "for info in users.values():\n",
    "    for follower in info.get(\"followers\", []):\n",
    "        if follower not in usernames_seen:\n",
    "            follower_depth = max_depth + 1\n",
    "            nodes.append({\n",
    "                \"id\": follower,\n",
    "                \"name\": follower,\n",
    "                \"img\": f\"./src/{follower}.jpg\",\n",
    "                \"url\": f\"https://github.com/{follower}\",\n",
    "                \"depth\": follower_depth\n",
    "            })\n",
    "            usernames_seen.add(follower)\n",
    "            username_to_depth[follower] = follower_depth\n",
    "\n",
    "# Create edges\n",
    "links = []\n",
    "for username, info in users.items():\n",
    "    for follower in info.get(\"followers\", []):\n",
    "        links.append({\n",
    "            \"source\": follower,\n",
    "            \"target\": username\n",
    "        })\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"nodes\": nodes, \"links\": links}, f, indent=2)\n",
    "\n",
    "print(f\"Graph data written to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd0c6f9e-2e44-41fb-81ec-0707d7db68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve the file for the html to be able to load the json\n",
    "# For Python 3.x\n",
    "#python -m http.server 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98da28e-d0fa-45e7-bb0e-a0fd209c36a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
